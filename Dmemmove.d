/*
Boost Software License - VeRSIon 1.0 - August 17th, 2003
Permission is hereby granted, free of charge, to any person or organization
obtaining a copy of the software and accompanying documentation covered by
this license (the "Software") to use, reproduce, display, distribute,
execute, and transmit the Software, and to prepare derivative works of the
Software, and to permit third-parties to whom the Software is furnished to
do so, all subject to the following:
The copyright notices in the Software and this entire statement, including
the above license grant, this restriction and the following disclaimer,
must be included in all copies of the Software, in whole or in part, and
all derivative works of the Software, unless such copies or derivative
works are solely in the form of machine-executable object code generated by
a source language processor.
THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE, TITLE AND NON-INFRINGEMENT. IN NO EVENT
SHalL THE COPYRIGHT HOLDERS OR ANYONE DISTRIBUTING THE SOFTWARE BE LIABLE
FOR ANY DAMAGES OR OTHER LIABILITY, WHETHER IN CONTRACT, TORT OR OTHERWISE,
ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
DEalINGS IN THE SOFTWARE.
*/

import core.stdc.string: memmove;
import S_struct;

void Cmemmove(T)(T *dst, const T *src)
{
    pragma(inline, true)
    memmove(dst, src, T.sizeof);
}

// IMPORTANT(stefanos): memmove is supposed to return the dest
void Dmemmove(T)(T *dst, const T *src)
{
    static if (T.sizeof == 64) {
        import core.simd: void16, loadUnaligned, storeUnaligned;
        pragma(inline, true);
        storeUnaligned(cast(void16*)(cast(void*)dst), loadUnaligned(cast(const void16*)(cast(void*)src)));
        storeUnaligned(cast(void16*)(cast(void*)dst+16), loadUnaligned(cast(const void16*)(cast(void*)src+16)));
        storeUnaligned(cast(void16*)(cast(void*)dst+32), loadUnaligned(cast(const void16*)(cast(void*)src+32)));
        storeUnaligned(cast(void16*)(cast(void*)dst+48), loadUnaligned(cast(const void16*)(cast(void*)src+48)));
        return;
    } else {
        asm pure nothrow @nogc {
            naked;
            mov     RDX, T.sizeof;
            cmp     RDX, 64;
            jb     LSMALL;
            add     RSI, RDX;
            add     RDI, RDX;
            // IMPORTANT(stefanos): Align destination (i.e. the writes)
            mov     ECX, EDI;
            and     ECX, 0x1f;
            je      L4;
            vmovdqu YMM0, [RSI-0x20];
            vmovdqu [RDI-0x20], YMM0;
            // src -= mod
            sub    RSI, RCX;
            // dst -= mod
            sub    RDI, RCX;
            // n -= mod
            sub    RDX, RCX;
        align 16;
        L4:
            // Because of the above, (at least) the loads
            // are 32-byte aligned.
            vmovdqu YMM0, [RSI-0x20];
            vmovdqu YMM1, [RSI-0x40];
            vmovdqu YMM2, [RSI-0x60];
            vmovdqu YMM3, [RSI-0x80];

            vmovdqu [RDI-0x20], YMM0;
            vmovdqu [RDI-0x40], YMM1;
            vmovdqu [RDI-0x60], YMM2;
            vmovdqu [RDI-0x80], YMM3;
            sub    RSI, 128;
            sub    RDI, 128;
            sub    RDX, 128;
            cmp    RDX, 128;
            jge    L4;
            vzeroupper;
        L2:
            test   RDX, RDX;
            je     L3;
            // if (n != 0)  -> copy the remaining < 128 bytes
            vmovdqu YMM0, [RSI-0x20];
            vmovdqu YMM1, [RSI-0x40];
            vmovdqu [RDI-0x20], YMM0;
            vmovdqu [RDI-0x40], YMM1;
            neg     RDX;
            add     RDX, 0x40;
            add     RSI, RDX;
            add     RDI, RDX;
            vmovdqu YMM0, [RSI-0x20];
            vmovdqu YMM1, [RSI-0x40];
            vmovdqu [RDI-0x20], YMM0;
            vmovdqu [RDI-0x40], YMM1;
        L3:
            vzeroupper;
            ret;

            // For less than 64. Hopefully, that will eventually be hanDLed
            // statically.
            
            // This is much like a switch fall-through.
        LSMALL:
            test        EDX, 32;
            jz          LSMALL_16;
            sub         EDX, 32;
            movups      XMM0, [RSI+RDX+0x10];
            movups      XMM1, [RSI+RDX];
            movups      [RDI+RDX+0x10], XMM0;
            movups      [RDI+RDX], XMM1;

            //vmovdqu     YMM0, [RSI+RDX];
            //vmovdqu     [RDI+RDX], YMM0;
        LSMALL_16:
            test        EDX, 16;
            jz          LSMALL_8;
            sub         EDX, 16;
            movups      XMM0, [RSI+RDX];
            movups      [RDI+RDX], XMM0;
        LSMALL_8:
            test        EDX, 8;
            jz          LSMALL_4;
            sub         EDX, 8;
            mov         RAX, [RSI+RDX];
            mov         [RDI+RDX], RAX;
        LSMALL_4:
            test        EDX, 4;
            jz          LSMALL_2;
            sub         EDX, 4;
            mov         EAX, [RSI+RDX];
            mov         [RDI+RDX], EAX;
            jz          LEND;
        LSMALL_2:
            test        EDX, 2;
            jz          LSMALL_1;
            sub         EDX, 2;
            movzx       EAX, word ptr [RSI+RDX];
            mov         word ptr [RDI+RDX], AX;
        LSMALL_1:
            test        EDX, 1;
            jz          LEND;
            movzx       EAX, byte ptr [RSI];
            mov         byte ptr [RDI], AL;
        LEND:
            ret;
        }
    }
}
